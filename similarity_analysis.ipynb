{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing President Trump's Tweets and Executive Office Activity using NLP\n",
    "****\n",
    "This notebook compares the documents published by the Executive Office of the President (of the United States of America) from January 20, 2017, to December 8th, 2017, and his tweets during the same time period. The data wrangling steps can be found in this GitHub repo (https://github.com/mtchem/Twitter-Politics/blob/master/Data_Wrangle.ipynb)\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "# imports for cosine similarity with NMF\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction import text \n",
    "# imports for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# special matplotlib argument for in notebook improved plots\n",
    "from matplotlib import rcParams\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Part 1: Data Wrangle\n",
    "#### Load and transform the data for analysis\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3e898a7c87f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# load federal document data from pickle file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfed_reg_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'data/fed_reg_data.pickle'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfed_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfed_reg_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# load twitter data from csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtwitter_file_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'data/twitter_01_20_17_to_3-2-18.pickle'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# load federal document data from pickle file\n",
    "fed_reg_data = r'data/fed_reg_data.pickle'\n",
    "fed_data = pd.read_pickle(fed_reg_data)\n",
    "# load twitter data from csv\n",
    "twitter_file_path = r'data/twitter_01_20_17_to_3-2-18.pickle'\n",
    "twitter_data = pd.read_pickle(twitter_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the index (date), to a column\n",
    "fed_data['date'] = fed_data.index\n",
    "twitter_data['date'] = twitter_data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine data for analysis\n",
    "<p> Create a dataframe that contains:\n",
    "<ul>\n",
    "    <li> Each document, from both data sets, as a string </li>\n",
    "    <li> The date the text was published </li>\n",
    "    <li> A label for the type of document (0= twitter doc, 1= federal doc) </li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Federal  Register / Vol.  82,  No.  161 / Tues...</td>\n",
       "      <td>2017-08-22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Federal  Register / Vol.  82,  No.  188 / Frid...</td>\n",
       "      <td>2017-09-29</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42706 \\n\\nFederal  Register / Vol.  82,  No.  ...</td>\n",
       "      <td>2017-09-11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts       date  label  ID\n",
       "0  Federal  Register / Vol.  82,  No.  161 / Tues... 2017-08-22      1   0\n",
       "1  Federal  Register / Vol.  82,  No.  188 / Frid... 2017-09-29      1   1\n",
       "2  42706 \\n\\nFederal  Register / Vol.  82,  No.  ... 2017-09-11      1   2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep text strings and rename columns\n",
    "fed = fed_data[['str_text', 'date']].rename({'str_text': 'texts'}, axis = 'columns')\n",
    "tweet = twitter_data[['text', 'date']].rename({'text': 'texts'}, axis = 'columns')\n",
    "\n",
    "# Add a label for the type of document (Tweet = 0, Fed = 1)\n",
    "tweet['label'] = 0\n",
    "fed['label'] = 1\n",
    "\n",
    "# concatinate the dataframes\n",
    "comb_text = pd.concat([fed,tweet])\n",
    "\n",
    "# Re_index so that each doc has a unique id_number\n",
    "comb_text = comb_text.reset_index()\n",
    "comb_text['ID'] = range(0,len(comb_text))\n",
    "\n",
    "# Look at the dataframe to make sure it works\n",
    "comb_text = comb_text[['texts','date','label', 'ID']]\n",
    "comb_text.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform text data into a word-frequency array\n",
    "<p> Computers cannot understand a text like humans, so in order to analyze text data, I first need to make every word a feature (column) in an array, where each document (row) is represented by a weighted* frequency of each word (column) they contain. An example text and array are shown below.\n",
    "</p>\n",
    "\n",
    "<p> Using Scikit Learn to create a word-frequency array:\n",
    "<ul>\n",
    "    <li> Define list of stop words (nonsense or non-meaninful words, such as 'the', 'a', 'of', 'q34fqwer3'). </li>\n",
    "    <li> Instantiate a tf-idf object (term frequency-inverse document frequency reweighting), that removes the stop words, and filters any word that appears in 99% of the documents</li>\n",
    "    <li> Create a matrix representation of the documents </li>\n",
    "    <li> Create list of the words each feature(column) represents </li>\n",
    "    <li> Print a list of the excluded words </li>\n",
    "</ul>\n",
    "</p>\n",
    "\n",
    "*Weighting the word frequencies lowers the importance that very frequently used domain-specific words are considered less important during the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nonsense words, and standard words like proclimation and dates\n",
    "more_stop = set(['presidential', 'documents', 'therfore','i','donald', 'j', 'trump', 'president', 'order', \n",
    "                 'authority', 'vested', 'articles','january','february','march','april','may','june','july','august','september','october',\n",
    "                 'november','december','jan','feb','mar','apr','jun','jul','aug','sep','oct','nov','dec',\n",
    "                 '2017','2018','act','agencies','agency','wh','rtlwanjjiq','pmgil08opp','blkgzkqemw','qcdljff3wn','erycjgj23r ','fzep1e9mo7','m0hmpbuz6c','rdo6jt2pip','kyv866prde','aql4jlvndh',\n",
    "             'tx5snacaas','t0eigo6lp8','jntoth0mol','8b8aya7v1s', 'x25t9tqani','q7air0bum2','ypfvhtq8te','ejxevz3a1r','1zo6zc2pxt',\n",
    "             'strciewuws','lhos4naagl','djlzvlq6tj', 'theplumlinegs', '3eyf3nir4b','cbewjsq1a3','lvmjz9ax0u',\n",
    "             'dw0zkytyft','sybl47cszn','6sdcyiw4kt','¼ï','yqf6exhm7x','cored8rfl2','6xjxeg1gss','dbvwkddesd',\n",
    "             'ncmsf4fqpr','twunktgbnb','ur0eetseno','ghqbca7yii','cbqrst4ln4','c3zikdtowc','6snvq0dzxn','ekfrktnvuy',\n",
    "             'k2jakipfji','œthe ','p1fh8jmmfa','vhmv7qoutk','mkuhbegzqs','ajic3flnki','mvjbs44atr',\n",
    "             'wakqmkdpxa','e0bup1k83z','ðÿ','ºðÿ','µðÿ','eqmwv1xbim','hlz48rlkif','td0rycwn8c','vs4mnwxtei','75wozgjqop',\n",
    "             'e1q36nkt8g','u8inojtf6d','rmq1a5bdon','5cvnmhnmuh','pdg7vqqv6m','s0s6xqrjsc','5cvnmhnmuh','wlxkoisstg',\n",
    "             'tmndnpbj3m','dnzrzikxhd','4qckkpbtcr','x8psdeb2ur','fejgjt4xp9','evxfqavnfs','aty8r3kns2','pdg7vqqv6m','nqhi7xopmw',\n",
    "             'lhos4naagl','32tfova4ov','zkyoioor62','np7kyhglsv','km0zoaulyh','kwvmqvelri','pirhr7layt',\n",
    "             'v3aoj9ruh4','https','cg4dzhhbrv','qojom54gy8','75wozgjqop','aty8r3kns2','nxrwer1gez','rvxcpafi2a','vb0ao3s18d',\n",
    "             'qggwewuvek','ddi1ywi7yz','r5nxc9ooa4','6lt9mlaj86','1jb53segv4','vhmv7qoutk','i7h4ryin3h',\n",
    "             'aql4jlvndh','yfv0wijgby','nonhjywp4j','zomixteljq','iqum1rfqso','2nl6slwnmh','qejlzzgjdk',\n",
    "             'p3crvve0cy','s0s6xqrjsc','gkockgndtc','2nl6slwnmh','zkyoioor62','clolxte3d4','iqum1rfqso',\n",
    "             'msala9poat','p1f12i9gvt','mit2lj7q90','qejlzzgjdk','pjldxy3hd9','vjzkgtyqb9','b2nqzj53ft',\n",
    "             'tpz7eqjluh','enyxyeqgcp','avlrroxmm4','2kuqfkqbsx','kwvmqvelri','œi','9lxx1iqo7m','vdtiyl0ua7',\n",
    "             'dmhl7xieqv','3jbddn8ymj','gysxxqazbl','ðÿž','tx5snacaas','4igwdl4kia','kqdbvxpekk','1avysamed4',\n",
    "             'cr4i8dvunc','bsp5f3pgbz','rlwst30gud','rlwst30gud','g4elhh9joh', '2017', 'January', 'kuqizdz4ra', \n",
    "             'nvdvrrwls4','ymuqsvvtsb', 'rgdu9plvfk','bk7sdv9phu','b5qbn6llze','xgoqphywrt ','hscs4y9zjk ',\n",
    "             'soamdxxta8','erycjgj23r','ryyp51mxdq','gttk3vjmku','j882zbyvkj','9pfqnrsh1z','ubbsfohmm7',\n",
    "             'xshsynkvup','xwofp9z9ir','1iw7tvvnch','qeeknfuhue','riqeibnwk2','seavqk5zy5','7ef6ac6kec',\n",
    "             'htjhrznqkj','8vsfl9mzxx','xgoqphywrt','zd0fkfvhvx','apvbu2b0jd','mstwl628xe','4hnxkr3ehw','mjij7hg3eu',\n",
    "             '1majwrga3d','x6fuuxxyxe','6eqfmrzrnv','h1zi5xrkeo','kju0moxchk','trux3wzr3u','suanjs6ccz',\n",
    "             'ecf5p4hjfz','m5ur4vv6uh','8j7y900vgk','7ef6ac6kec','d0aowhoh4x','aqqzmt10x7','zauqz4jfwv',\n",
    "             'bmvjz1iv2a','gtowswxinv','1w3lvkpese','8n4abo9ihp','f6jo60i0ul','od7l8vpgjq','odlz2ndrta',\n",
    "             '9tszrcc83j','6ocn9jfmag','qyt4bchvur','wkqhymcya3','tp4bkvtobq','baqzda3s2e','March','April',\n",
    "             'op2xdzxvnc','d7es6ie4fy','proclamation','hcq9kmkc4e','rf9aivvb7g','sutyxbzer9','s0t3ctqc40','aw0av82xde'])\n",
    "# defines all stop words\n",
    "my_stop = text.ENGLISH_STOP_WORDS.union(more_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate TfidfVectorizer to remove common english words, and any word used in 99% of the documents\n",
    "tfidf = TfidfVectorizer(stop_words = my_stop , max_df = 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create matrix representation of all documents\n",
    "text_mat = tfidf.fit_transform(comb_text.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of feature words\n",
    "words = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excluded Words\n",
    "<p> \n",
    "    Below is a printed list of all of the excluded words.  I include this because I am not a political scientist or a linguist.  What I consider to be nonsense maybe important and you may want to modify this list.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'enough', 'thus', 'a', 'kuqizdz4ra', 'might', 'have', 'bmvjz1iv2a', 'become', 'thereupon', 'thin', 'found', 'nowhere', 'around', 'could', 'kyv866prde', 'ddi1ywi7yz', 'move', 'you', 'can', 'nxrwer1gez', 'm0hmpbuz6c', 'ypfvhtq8te', 'he', 'four', 'eg', '2018', 'amoungst', 'no', 's0s6xqrjsc', 'find', 'to', 'ur0eetseno', 'whether', 'x25t9tqani', 'any', 'sometime', 'evxfqavnfs', '2nl6slwnmh', 'cry', 'kqdbvxpekk', 'dbvwkddesd', 'system', 'k2jakipfji', 'htjhrznqkj', 'within', 'even', 'sutyxbzer9', 'riqeibnwk2', 'along', 'sometimes', 'before', 'down', 'top', 'seeming', 'therfore', 'pmgil08opp', 'will', 'wkqhymcya3', 'ten', 'whenever', 'three', 'dec', 'those', 'µðÿ', 'some', 'feb', 'sep', 'cbqrst4ln4', 'b2nqzj53ft', 'eqmwv1xbim', 'u8inojtf6d', 'together', 'co', 'wakqmkdpxa', 'km0zoaulyh', 'xgoqphywrt', 'now', 'them', 'fzep1e9mo7', 'vdtiyl0ua7', 'against', 'much', 'ourselves', 'up', 'april', 'be', 'is', 'third', 'kwvmqvelri', 'name', 'strciewuws', 'per', 'whereafter', 'nvdvrrwls4', 'empty', 'always', 'my', 'who', 'him', 'our', 'trump', 'iqum1rfqso', 'across', 'september', 'np7kyhglsv', 'therein', 'been', 'qojom54gy8', 'aty8r3kns2', 'de', 'fire', 'because', 'since', 'qejlzzgjdk', 'ours', 'avlrroxmm4', 'documents', 'during', 'one', 'odlz2ndrta', 'as', 'below', 'r5nxc9ooa4', 'january', 'though', 'g4elhh9joh', 'e0bup1k83z', 'cg4dzhhbrv', 'x6fuuxxyxe', 'd0aowhoh4x', 'qeeknfuhue', 'formerly', 'order', 'ie', 'dmhl7xieqv', 'twelve', 'its', 'that', 'this', 'eleven', 'wh', 'xshsynkvup', 'very', 'next', 'dnzrzikxhd', 'yfv0wijgby', 'op2xdzxvnc', 'aw0av82xde', 'too', 'yourselves', 'clolxte3d4', 'his', 'november', 'elsewhere', 'on', 'which', 'into', 'January', 'many', 'about', 'bill', 'donald', '¼ï', '4qckkpbtcr', 'fejgjt4xp9', 'amount', 'c3zikdtowc', '6lt9mlaj86', 'whence', 'hereupon', 'rather', 'mit2lj7q90', 'and', 'ryyp51mxdq', 'each', 'b5qbn6llze', 'again', 'are', 'most', 'out', 'jntoth0mol', 'august', 'presidential', 'agency', '6sdcyiw4kt', 'f6jo60i0ul', 'six', 'the', 'yourself', 'noone', 'lvmjz9ax0u', 'himself', 'couldnt', 'except', 'go', 'off', 'see', 'was', 'when', '8n4abo9ihp', 'herself', 'hence', 'of', 'had', 'were', 'mostly', 'why', 'vs4mnwxtei', 'tpz7eqjluh', '8j7y900vgk', 'least', 'nov', 'zd0fkfvhvx', '2kuqfkqbsx', 'vjzkgtyqb9', 'whereby', 'xwofp9z9ir', 'proclamation', 'has', 'so', 'onto', 'themselves', 'namely', 'after', 'anything', 'became', 'full', 'moreover', 'thru', 'also', 'enyxyeqgcp', 'cr4i8dvunc', 'od7l8vpgjq', 'perhaps', 'then', 'ejxevz3a1r', 'nobody', 'give', 'it', 'latter', 'describe', 'over', 'there', 'herein', 'without', 'not', 're', 'i7h4ryin3h', 'meanwhile', 'ncmsf4fqpr', 'becoming', 'if', 'therefore', 'further', '9lxx1iqo7m', 'baqzda3s2e', '9pfqnrsh1z', 'beforehand', 'may', 'where', 'yqf6exhm7x', 'hscs4y9zjk ', 'apvbu2b0jd', 'qyt4bchvur', 'yours', 'alone', 'wherein', 'march', 'agencies', 'among', 'former', 'in', 'back', 'anyone', '6ocn9jfmag', 'none', 'erycjgj23r', 'beside', 'qcdljff3wn', 'mvjbs44atr', 'vhmv7qoutk', 'gtowswxinv', 'please', 'toward', 'indeed', 'aug', '3jbddn8ymj', 'between', 'œthe ', 'besides', 'often', 'several', 'authority', 'rtlwanjjiq', 'trux3wzr3u', 'nevertheless', 'aql4jlvndh', '7ef6ac6kec', 'rdo6jt2pip', 'theplumlinegs', 'jul', 'erycjgj23r ', 'both', 'apr', 'nonhjywp4j', 'am', 'act', 'p1f12i9gvt', 'hcq9kmkc4e', 'yet', 'few', 'more', '1jb53segv4', 'five', 'part', 'what', 'ymuqsvvtsb', 'everyone', 'their', 'mkuhbegzqs', 'own', 'done', 'front', 'your', 'how', 'us', 'same', 'gttk3vjmku', 'we', 'jun', 'ðÿ', 'mstwl628xe', 'itself', '1w3lvkpese', 'seems', 'december', 'eight', 'tx5snacaas', 'wlxkoisstg', 'i', 'wherever', 'p1fh8jmmfa', 'whither', 'would', '75wozgjqop', 'an', 'still', 'must', 'vb0ao3s18d', 'these', 'zauqz4jfwv', 's0t3ctqc40', 'pdg7vqqv6m', 'for', 'july', 'nine', 'ghqbca7yii', 'rlwst30gud', 'beyond', 'keep', 'they', 'q7air0bum2', 'ltd', 'oct', 'somewhere', 'all', 'behind', 'anyhow', 'but', 'hundred', 'sixty', 'seem', 'whole', 'forty', 'j882zbyvkj', 'sybl47cszn', 'president', '6xjxeg1gss', 'ajic3flnki', 'call', 'mine', 'e1q36nkt8g', '1majwrga3d', 'h1zi5xrkeo', 'tp4bkvtobq', 'hasnt', 'nothing', '8vsfl9mzxx', 'here', 'zkyoioor62', 'others', 'take', 'ecf5p4hjfz', 'although', 'she', 'bottom', 'sincere', 'twenty', 'than', 'td0rycwn8c', 'however', 'myself', 'dw0zkytyft', 'd7es6ie4fy', 'something', 'afterwards', 'almost', 'œi', 'should', 'p3crvve0cy', 'blkgzkqemw', 'zomixteljq', 'from', 'only', 'another', 'well', 'twunktgbnb', '6snvq0dzxn', 'seavqk5zy5', 'put', 'ekfrktnvuy', 'gysxxqazbl', 'm5ur4vv6uh', 'aqqzmt10x7', 'soamdxxta8', 'hers', 'either', 'february', 'msala9poat', 'con', 'with', 'somehow', 'amongst', 'due', 'mill', 'someone', 'two', 'ðÿž', 'rgdu9plvfk', 'hereafter', 'me', 'seemed', 'nor', 'throughout', 'March', 'jan', 'inc', 'every', 'such', '4hnxkr3ehw', 'anywhere', 'fifty', 'whoever', '1avysamed4', 'djlzvlq6tj', 'gkockgndtc', 'while', 'do', '32tfova4ov', 'thereby', 'made', '4igwdl4kia', 'never', 'cannot', 'at', 'ubbsfohmm7', 'her', 'j', 'first', 'under', 'nqhi7xopmw', 'whereas', '9tszrcc83j', 'suanjs6ccz', 't0eigo6lp8', 'already', 'whose', 'by', 'pjldxy3hd9', 'fifteen', 'being', '1zo6zc2pxt', 'April', 'kju0moxchk', 'via', 'until', 'october', 'whereupon', 'otherwise', '5cvnmhnmuh', 'mjij7hg3eu', 'whatever', 'side', 'thereafter', 'less', 'once', 'ºðÿ', 'etc', 'hlz48rlkif', 'else', 'ever', 'qggwewuvek', 'fill', '3eyf3nir4b', '2017', 'anyway', 'serious', 'everything', 'tmndnpbj3m', 'neither', 'xgoqphywrt ', 'lhos4naagl', 'through', 'or', '6eqfmrzrnv', 'cbewjsq1a3', 'v3aoj9ruh4', 'mar', 'everywhere', 'get', 'vested', 'rmq1a5bdon', 'last', 'latterly', 'rf9aivvb7g', 'towards', 'whom', '8b8aya7v1s', 'upon', 'https', 'above', 'hereby', 'cored8rfl2', 'thence', '1iw7tvvnch', 'interest', 'other', 'cant', 'articles', 'un', 'thick', 'june', 'show', 'x8psdeb2ur', 'detail', 'rvxcpafi2a', 'pirhr7layt', 'bsp5f3pgbz', 'becomes', 'bk7sdv9phu'})\n"
     ]
    }
   ],
   "source": [
    "# print excluded words from the matrix features\n",
    "print(tfidf.get_stop_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "# Part 2: Analysis\n",
    "#### Use unsupervised machine learning to analyze both President Trump's tweets, official presidential actions and explore any correlation between the two\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2A: Determine the document's topics\n",
    "<p> Model the documents with non-zero matrix factorization (NMF):\n",
    "<ul>\n",
    "    <li> Instantiate NMF model with 260 components (1/10th the number of documents) and initialized with Nonnegative Double Singular Value Decomposition (NNDSVD, better for sparseness)</li>\n",
    "    <li> Fit(learn the NMF model for the tf-idf matrix) model</li>\n",
    "    <li> Transform the model, which applies the fit to the matirix </li>\n",
    "    <li> Make a dataframe with the NMF components for each word </li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "NMF_model = NMF(n_components=260 , init = 'nndsvd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0,\n",
       "  max_iter=200, n_components=260, random_state=None, shuffle=False,\n",
       "  solver='cd', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "NMF_model.fit(text_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the text frequecy matrix using the fitted NMF model\n",
    "nmf_features = NMF_model.transform(text_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with words as a columns, NMF components as rows\n",
    "components_df = pd.DataFrame(NMF_model.components_, columns = words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2B: Find the top 5 topic words (components) for each document\n",
    "<p> Using the components dataframe create a dictionary with components as keys, and top words as values:\n",
    "<ul>\n",
    "    <li> Make an empty dictionary and loop through each row of NMF components</li>\n",
    "    <li> Add to the dictionary where the key is the NMF component and the value is the topic words for that component (the column names with the largest component values)</li>\n",
    "\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary with the key = component, value = top 5 words\n",
    "topic_dict = {}\n",
    "for i in range(0,260):\n",
    "    component = components_df.iloc[i, :]\n",
    "    topic_dict[i] = component.nlargest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['states', 'united', '11', 'fr', '4790'], dtype='object')\n",
      "Index(['shall', 'law', 'sec', 'federal', 'section'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# look at a few of the component topics\n",
    "print(topic_dict[0].index)\n",
    "print(topic_dict[7].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2C: Cosine Similarity\n",
    "\n",
    "<p> The informal and non-regular grammar used in tweets makes a direct comparison with documents published by the Executive Office, which uses formal vocabulary and grammar, difficult. Therefore, I will use the metric, cosine similarity, which compares the distance between feature vectors, instead of direct word comparison. Higher cosine similarities between two documents indicate greater topic similarity.\n",
    "</p>\n",
    "\n",
    "<p>Calculating cosine similarities of NMF features:\n",
    "<ul>\n",
    "    <li> Normalize NMF features (calculated in part 2A)</li>\n",
    "    <li> Create dataframe where each row contains the normalized NMF features for a document and its ID number</li>\n",
    "    <li> Look at each row(decomposed article) and calculate its cosine similarity to all other document's normalized NMF features </li>\n",
    "    <li> Create a dictionary where the key is the document ID, and the value is a pandas series of the 5 most similar documents (including its self)</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize previouly found nmf features\n",
    "norm_features = normalize(nmf_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe of document's NMF features, where rows are documents and columns are NMF components\n",
    "df_norms = pd.DataFrame(norm_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize empty dictionary\n",
    "similarity_dict= {}\n",
    "# loop through each row of the df_norms dataframe\n",
    "for i in range(len(norm_features)):\n",
    "    # isolate one row, by ID number\n",
    "    row = df_norms.loc[i]\n",
    "    # calculate the top cosine similarities\n",
    "    top_sim = (df_norms.dot(row)).nlargest()\n",
    "    # append results to dictionary\n",
    "    similarity_dict[i] = (top_sim.index, top_sim) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "## Part 3: Use the cosine similarity results to explore how (or if) President Trump's tweets and official actions correlate\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3A: Find Twitter documents that have at least one federal document in its top 5 cosine similarity scores (and vice versa)\n",
    "<p>  Using the results of part 2C, find the types of documents are the most similar, then sum the labels (0=twitter, 1= federal document). If similar documents are a mix of tweets and federal documents, then the sum of their value will be either 1,2,3 or 4.\n",
    "<ul>\n",
    "    <li> Create a dataframe with the document ID number as the index and the document type label (tweet = 0, fed_doc = 1)</li>\n",
    "    <li> Loop through each document in the dataframe and use the similarity dictionary to find the list of most similar document ID numbers and the sum of the similarity scores</li>\n",
    "    <li> For each list of similar documents, sum the value of the document type labels. If the sum value is 1, 2, 3, or 4, that means there are both tweets and federal documents in the group</li>\n",
    " \n",
    "</ul>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataframe with document ID and labels\n",
    "doc_label_df = comb_text[['label', 'ID']].copy().set_index('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inialize list for the sum of all similar documents label\n",
    "label_sums =[]\n",
    "similarity_score_sum = []\n",
    "# loop through all of the documents\n",
    "for doc_num in doc_label_df.index:\n",
    "    # sum the similarity scores\n",
    "    similarity_sum = similarity_dict[doc_num][1].sum()\n",
    "    similarity_score_sum.append(similarity_sum)\n",
    "    \n",
    " \n",
    "    #find the list of similar documents\n",
    "    similar_doc_ID_list = list(similarity_dict[doc_num][0])    \n",
    "    # loop through labels\n",
    "    s_label = 0\n",
    "    for ID_num in similar_doc_ID_list:\n",
    "        # sum the label values for each similar document\n",
    "        s_label = s_label + doc_label_df.loc[ID_num].label\n",
    "        \n",
    "    # append the sum of the labels for ONE document\n",
    "    label_sums.append(s_label)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the similarity score sum to dataframe as separate column\n",
    "doc_label_df['similarity_score_sum'] = similarity_score_sum\n",
    "\n",
    "# add the similar document's summed label value to the dataframe as a separate column\n",
    "doc_label_df['sum_of_labels'] = label_sums        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3B: Look at the topics of tweets that have similar federal documents (and vice versa)\n",
    "<p>  Isolate documents with mixed types of similar documents and high similarity scores\n",
    "<ul>\n",
    "    <li> Filter dataframe to include only top_similar_label_sums with a value of 1, 2, 3, or 4</li>\n",
    "    <li> Filter again to only include groups with high combinded similarity scores</li>\n",
    "    <li> Remove and duplicate groups </li>\n",
    " \n",
    "</ul>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID  label  similarity_score_sum  sum_of_labels\n",
      "0   0      1              3.819105              2\n",
      "1   1      1              3.324981              3\n",
      "2   9      1              4.859847              1\n",
      "3  18      1              3.339872              1\n",
      "4  24      1              4.563310              4\n",
      "293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\py36\\lib\\site-packages\\ipykernel\\__main__.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Filter dataframe for federal documents with similar tweets, and vice versa\n",
    "df_filtered = doc_label_df[doc_label_df['sum_of_labels'] != 0][doc_label_df['sum_of_labels'] != 5].copy().reset_index()\n",
    "\n",
    "# Make sure it worked\n",
    "print(df_filtered.head())\n",
    "print(len(df_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the ones that have all top 5 documents with a cosine similarity score of 0.9 or above.  \n",
    "#The sum of scores need to be 4.6 or higher\n",
    "similar_score_min = 4.6\n",
    "highly_similar = df_filtered[df_filtered.similarity_score_sum >= similar_score_min]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicate highly similar groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all the group lists\n",
    "doc_groups = []\n",
    "for doc_id in highly_similar.ID:\n",
    "    doc_groups.append(sorted(list(similarity_dict[doc_id][0])))\n",
    "\n",
    "# make the interior lists tuples, then make a set of them\n",
    "unique_groups = set([tuple(x) for x in doc_groups])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(4, 5, 20, 157, 2090),\n",
       " (4, 20, 2040, 2069, 2084),\n",
       " (9, 221, 227, 240, 2256),\n",
       " (9, 221, 1179, 1690, 2256),\n",
       " (9, 694, 820, 1690, 2256),\n",
       " (9, 694, 1690, 2256, 2578),\n",
       " (25, 127, 174, 863, 1696),\n",
       " (25, 174, 863, 1696, 1845),\n",
       " (28, 71, 229, 248, 2576),\n",
       " (28, 204, 229, 248, 2576),\n",
       " (28, 229, 233, 248, 2576),\n",
       " (28, 229, 248, 2571, 2576),\n",
       " (47, 84, 130, 1806, 2070),\n",
       " (49, 205, 428, 1578, 2312),\n",
       " (49, 205, 1578, 1917, 2312),\n",
       " (49, 205, 1672, 1917, 2312),\n",
       " (49, 363, 428, 1578, 2463),\n",
       " (71, 95, 229, 248, 2576),\n",
       " (74, 694, 820, 1838, 2508),\n",
       " (82, 1545, 1682, 1785, 2532),\n",
       " (84, 102, 131, 170, 2070),\n",
       " (102, 131, 170, 478, 2380),\n",
       " (131, 170, 478, 479, 2380),\n",
       " (131, 328, 478, 479, 2380),\n",
       " (131, 478, 479, 555, 2380),\n",
       " (170, 478, 479, 1806, 2070),\n",
       " (229, 248, 1526, 2571, 2576),\n",
       " (229, 248, 1743, 2020, 2576),\n",
       " (251, 497, 1689, 1778, 2120),\n",
       " (251, 1653, 1689, 1916, 2120),\n",
       " (260, 414, 922, 1135, 1180),\n",
       " (260, 414, 1093, 1135, 1180),\n",
       " (260, 773, 1093, 1118, 1135),\n",
       " (260, 1093, 1095, 1118, 1135),\n",
       " (260, 1093, 1118, 1135, 2111)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3C: Manually look at the documents.  Are they similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Components = 100 ,  Highly similar score = 4.9\n",
    "<p> Four of the 5 unique groups are basically the same \n",
    "    <ul> {(58, 80, 105, 149, 1139),\n",
    "         (58, 80, 126, 149, 1139),\n",
    "         (58, 80, 126, 185, 1139),\n",
    "         (58, 80, 149, 185, 1139),\n",
    "         (131, 170, 478, 479, 2044)}\n",
    "    </ul>\n",
    "    \n",
    "    Thoses components (58, 80, 105, 126, 149, 185, 1139) are all about national emergencies.  The 5 group is about national security and national emergencies\n",
    "    \n",
    "</p>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Components = 260 , Highly similar cutoff score = 4.6\n",
    "\n",
    "6 unique groups can be further distilled to one set (27, 28, 229, 248, 196, 203,2576, 2546, 204, 1151, 1892)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @VP: Our President is choosing to put American jobs, American consumers, American energy, and American industry first. https://t.co/y2Opâ€¦\n",
      "Federal  Register \n",
      "\n",
      "Vol.  82,  No.  84 \n",
      "\n",
      "Wednesday,  May  3,  2017 \n",
      "\n",
      "Title  3— \n",
      "\n",
      "The  President \n",
      "\n",
      "Presidential Documents\n",
      "\n",
      "20795 \n",
      "\n",
      "Proclamation  9595  of  April  28,  2017 \n",
      "\n",
      "Asian  American  and  Pacific  Islander  Heritage  Month,  2017 \n",
      "\n",
      "By  the  President  of  the  United  States  of  America \n",
      "\n",
      "A  Proclamation \n",
      "This  month,  we  celebrate  Asian  American  and  Pacific  Islander  Heritage \n",
      "Month, and we recognize the achievements and contributions of Asian Ameri-\n",
      "cans and Pacific Islanders that enrich our Nation. \n",
      "Asian  Americans  and  Pacific  Islanders  have  distinguished  themselves  in \n",
      "the  arts,  literature,  and  sports.  They  are  leading  researchers  in  science, \n",
      "medicine, and technology; dedicated teachers to our Nation’s children; inno-\n",
      "vative  farmers  and  ranchers;  and  distinguished  lawyers  and  government \n",
      "leaders. \n",
      "Dr. Sammy Lee, a Korean American who passed away last December, exem-\n",
      "plified  the  spirit  of  this  month.  Dr.  Lee  was  the  first  Asian  American  man \n",
      "to  win  an  Olympic  gold  medal,  becoming  a  platform  diving  champion  at \n",
      "the 1948 London Olympics only 1 year after graduating from medical school. \n",
      "To  fulfill  his  dreams,  Dr.  Lee  overcame  several  obstacles,  including  his \n",
      "local  childhood  pool’s  policy  of  opening  to  minorities  only  once  per  week. \n",
      "Later  in  life  he  was  subject  to  housing  discrimination  (even  after  8  years \n",
      "of  military  service).  Dr.  Lee  nevertheless  tirelessly  served  his  country  and \n",
      "community,  including  by  representing  the  United  States  at  the  Olympic \n",
      "Games, on behalf of several Presidents. \n",
      "Katherine  Sui  Fun  Cheung  also  embodied  the  spirit  of  this  month.  In  1932, \n",
      "she  became  the  first  Chinese  American  woman  to  earn  a  pilot  license. \n",
      "At  the  time,  only  about  1  percent  of  pilots  in  the  United  States  were \n",
      "women. As a member of The Ninety-Nines, an organization of women pilots, \n",
      "she paved the way for thousands of women to take to the skies. \n",
      "There  are  more  than  20  million  Asian  Americans  and  Pacific  Islanders \n",
      "in  the  United  States.  Each  day,  through  their  actions,  they  make  America \n",
      "more  vibrant,  more  prosperous,  and  more  secure.  Our  Nation  is  particularly \n",
      "grateful to the many Asian Americans and Pacific Islanders who have served \n",
      "and  are  currently  serving  in  our  Armed  Forces,  protecting  the  Nation,  and \n",
      "promoting freedom and peace around the world. \n",
      "NOW,  THEREFORE,  I,  DONALD  J.  TRUMP,  President  of  the  United  States \n",
      "of  America,  by  virtue  of  the  authority  vested  in  me  by  the  Constitution \n",
      "and  the  laws  of  the  United  States,  do  hereby  proclaim  May  2017  as  Asian \n",
      "American  and  Pacific  Islander  Heritage  Month.  The  Congress,  by  Public \n",
      "Law  102–450,  as  amended,  has  also  designated  the  month  of  May  each \n",
      "year as ‘‘Asian/Pacific American Heritage Month.’’ I encourage all Americans \n",
      "to  learn  more  about  our  Asian  American,  Native  Hawaiian,  and  Pacific \n",
      "Islander  heritage,  and  to  observe  this  month  with  appropriate  programs \n",
      "and activities. \n",
      "\n",
      "0\n",
      "D\n",
      "h\n",
      "\n",
      " \n",
      "\n",
      "t\n",
      "i\n",
      "\n",
      " \n",
      "\n",
      "w\n",
      "D\n",
      "O\n",
      "R\n",
      "P\n",
      "1\n",
      "N\n",
      "V\n",
      "T\n",
      "P\n",
      "S\n",
      "3\n",
      "K\n",
      "S\n",
      "D\n",
      "n\n",
      "o\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "s\n",
      "a\n",
      "k\n",
      "s\n",
      "u\n",
      "a\n",
      "\n",
      "i\n",
      "l\n",
      "\n",
      "a\n",
      "b\n",
      "a\n",
      "s\n",
      "a\n",
      "\n",
      "VerDate Sep<11>2014  18:27 May 02, 2017 Jkt 241001 PO 00000 Frm 00003 Fmt 4705 Sfmt 4790 E:\\FR\\FM\\03MYD0.SGM 03MYD0\n",
      "\n",
      "\f",
      "20796 \n",
      "\n",
      "Federal  Register / Vol.  82,  No.  84 / Wednesday,  May  3,  2017 / Presidential  Documents \n",
      "\n",
      "IN  WITNESS  WHEREOF,  I  have  hereunto  set  my  hand  this  twenty-eighth \n",
      "day  of  April,  in  the  year  of  our  Lord  two  thousand  seventeen,  and  of \n",
      "the  Independence  of  the  United  States  of  America  the  two  hundred  and \n",
      "forty-first. \n",
      "\n",
      "[FR  Doc.  2017–09073 \n",
      "Filed  5–2–17;  11:15  am] \n",
      "Billing  code  3295–F7–P \n",
      "\n",
      "0\n",
      "D\n",
      "h\n",
      "\n",
      " \n",
      "\n",
      "t\n",
      "i\n",
      "\n",
      " \n",
      "\n",
      "w\n",
      "D\n",
      "O\n",
      "R\n",
      "P\n",
      "1\n",
      "N\n",
      "V\n",
      "T\n",
      "P\n",
      "S\n",
      "3\n",
      "K\n",
      "S\n",
      "D\n",
      "n\n",
      "o\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "s\n",
      "a\n",
      "k\n",
      "s\n",
      "u\n",
      "a\n",
      "\n",
      "i\n",
      "l\n",
      "\n",
      "a\n",
      "b\n",
      "a\n",
      "s\n",
      "a\n",
      "\n",
      "VerDate Sep<11>2014  18:27 May 02, 2017 Jkt 241001 PO 00000 Frm 00004 Fmt 4705 Sfmt 4790 E:\\FR\\FM\\03MYD0.SGM 03MYD0\n",
      "\n",
      "/\n",
      "\n",
      ">\n",
      "H\n",
      "P\n",
      "G\n",
      "<\n",
      "S\n",
      "P\n",
      "E\n",
      "p\n",
      "m\n",
      "u\n",
      "r\n",
      "T\n",
      "\n",
      ".\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "print(comb_text.texts.loc[1892])\n",
    "print(comb_text.texts.loc[27])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "<p>\n",
    "    There does seem to be some general similarities between President Trump's tweets and official federal action. However, the topics are quite vague.  Such as tweets about specific White House officals are grouped with the federal documents that define who is on different committees in the Executive Office. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
