{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import re\n",
    "import io\n",
    "import urllib\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect and Clean Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using http://www.trumptwitterarchive.com/archive , I downloaded all of Donald Trump's tweets from 01/01/2016 - 10/11/2017 9:19 am MST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath to twitter archive data.\n",
    "twitter_json = r'C:\\Users\\aregel\\Documents\\springboard\\Capstone_2\\data_wrangle\\data\\twitter_01_01_16_to_10-11-17.json' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pandas I will read the twitter json file, convert it to a dataframe, set the index to 'created at' as datetime objects, then write it to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the json data into a pandas dataframe\n",
    "tweet_data = pd.read_json(twitter_json)\n",
    "# set column 'created_at' to the index\n",
    "tweet_data.set_index('created_at', drop=True, inplace= True)\n",
    "# convert timestamp index to a datetime index\n",
    "pd.to_datetime(tweet_data.index)\n",
    "# write to csv file\n",
    "csv_file_path = r'C:\\Users\\aregel\\Documents\\springboard\\Capstone_2\\data_wrangle\\data\\twitter_01_01_16_to_10-11-17.csv' \n",
    "tweet_data.to_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Scrape Data from the Federal Register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 2016, 2017 url that contains all of the Executive Office of the President's published documents\n",
    "executive_office_url_2016 = r'https://www.federalregister.gov/index/2016/executive-office-of-the-president'\n",
    "executive_office_url_2017 = r'https://www.federalregister.gov/index/2017/executive-office-of-the-president'\n",
    "\n",
    "# scrape both websites' html to one list of url's containing the pdf documents\n",
    "pdf_urls = []\n",
    "for url in [executive_office_url_2016, executive_office_url_2017]:\n",
    "    response = requests.get(url)\n",
    "    pattern = re.compile(r'https:.*\\.pdf')\n",
    "    pdfs = re.findall(pattern, response.text)\n",
    "    pdf_urls.append(pdfs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert pdf to text from stack overflow (https://stackoverflow.com/questions/26494211/extracting-text-from-a-pdf-file-using-pdfminer-in-python/44476759#44476759)\n",
    "def convert_pdf_to_txt(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos = set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages,\n",
    "                                  password=password,\n",
    "                                  caching=caching,\n",
    "                                  check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes list of dates for each pdf\n",
    "def pdf_dates(lists_of_urls):\n",
    "    url_pattern = re.compile(r'\\d{4}-\\d{2}-\\d{2}')\n",
    "    date_list = []\n",
    "    for lists in pdf_urls:\n",
    "        for url in lists:\n",
    "            date = re.findall(url_pattern, url)\n",
    "            date_list.append(date)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes fed reg pdf url, returns a datetime object\n",
    "def pdf_date(url):\n",
    "    url_pattern = re.compile(r'\\d{4}-\\d{2}-\\d{2}')\n",
    "    date = re.findall(url_pattern, url_example)\n",
    "    date_obj = datetime.strptime(date[0],'%Y-%m-%d')\n",
    "    return date_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieves pdf from url, writes it to local drive\n",
    "pdf_path = r'C:\\Users\\aregel\\Desktop\\pdfs\\example.pdf'\n",
    "pdfFile = urllib.request.urlopen(url2)\n",
    "file = open(pdf_path, 'wb')\n",
    "file.write(pdfFile.read())\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
